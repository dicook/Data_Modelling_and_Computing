<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ETC1010: Data Modelling and Computing</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr.Â Nicholas Tierney &amp; Professor Di Cook" />
    <meta name="date" content="2019-09-20" />
    <link href="libs/remark-css/shinobi.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ETC1010: Data Modelling and Computing
## Lecture 8B: Text analysis and linear models
### Dr.Â Nicholas Tierney &amp; Professor Di Cook
### EBS, Monash U.
### 2019-09-20

---




class: bg-main1

# recap 

.huge[
- tidying up text
- use `genius_album()` to download lyrics of songs
- stop_words - (I, am, be, the, this, what, we, myself)
]

---
class: bg-main1

# Overview

.huge[
- interactions in modelling
- tidy text
]

---
class: bg-main1

# Adding interactions to the model

![](images/interaction.png)

---
class: bg-main1

# Remember linear regression?

&lt;img src="lecture-8b-slides_files/figure-html/unnamed-chunk-1-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---
class: bg-main1
# Interaction between quantitative and categorical variables

.huge[
- An interaction term is needed in a model if the linear relationship is different for the response vs quantitative variable for different levels of the categorical variable. 
- That is, a different *slope* needs to be used/estimated for each level. 
]

---
class: bg-main1

# Interaction between quantitative and categorical variables

.huge[
- Let's take a look at how this works for the [2015 OECD PISA data](http://www.oecd.org/pisa/data/2015database/).
- The question to be answered is whether more time spent studying science is associated with higher science scores, and how this varies with enjoyment of science.
]

---
class: bg-main1

# PISA data


```r
pisa_au &lt;- read_csv("data/pisa_au.csv")
pisa_au
```

```
## # A tibble: 14,530 x 44
##    state schtype yr    birthmonth birthyr gender  desk  room computer internet solarpanels
##    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      
##  1 QLD   Gov     Y10           10    1999 female     1     2        1        1 0036001    
##  2 QLD   Gov     Y10            2    2000 female     2     1        1        1 0036002    
##  3 QLD   Gov     Y10            3    2000 female     1     1        1        1 0036002    
##  4 QLD   Gov     Y10            7    1999 female     1     1        1        1 0036001    
##  5 QLD   Gov     Y10            8    1999 male       1     1        1        1 0036001    
##  6 QLD   Gov     Y10            3    2000 male       1     1        1        1 0036001    
##  7 QLD   Gov     Y10            3    2000 male       1     1        1        1 0036001    
##  8 QLD   Gov     Y10            1    2000 male       1     1        2        1 0036001    
##  9 QLD   Gov     Y10            6    1999 female     1     1        1        1 0036001    
## 10 QLD   Gov     Y10            9    1999 female     1     1        2        1 0036002    
## # â€¦ with 14,520 more rows, and 33 more variables: tvs &lt;dbl&gt;, cars &lt;dbl&gt;,
## #   music_instr &lt;dbl&gt;, books &lt;dbl&gt;, birthcnt &lt;dbl&gt;, mother_birthcnt &lt;dbl&gt;,
## #   father_birthcnt &lt;dbl&gt;, test_anxiety &lt;dbl&gt;, ambitious &lt;dbl&gt;, prefer_team &lt;dbl&gt;,
## #   make_friends_easy &lt;dbl&gt;, tardy &lt;dbl&gt;, science_fun &lt;dbl&gt;, breakfast &lt;dbl&gt;,
## #   work_pay &lt;dbl&gt;, sport &lt;dbl&gt;, internet_use &lt;dbl&gt;, install_software &lt;dbl&gt;,
## #   outhours_study &lt;dbl&gt;, math_time &lt;dbl&gt;, read_time &lt;dbl&gt;, science_time &lt;dbl&gt;,
## #   belong &lt;dbl&gt;, anxtest &lt;dbl&gt;, motivat &lt;dbl&gt;, language &lt;dbl&gt;, home_edres &lt;dbl&gt;,
## #   home_poss &lt;dbl&gt;, wealth &lt;dbl&gt;, stuweight &lt;dbl&gt;, math &lt;dbl&gt;, science &lt;dbl&gt;, read &lt;dbl&gt;
```

---
class: bg-main1

# PISA data


```r
pisa_au %&gt;%
  select(science_time,
         science,
         science_fun)
```

```
## # A tibble: 14,530 x 3
##    science_time science science_fun
##           &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;
##  1          210    590.          NA
##  2          165    557.           3
##  3          210    569.           2
##  4          210    529.           4
##  5          210    504.           3
##  6          210    473.          NA
##  7          135    496.          NA
##  8          280    336.           1
##  9          210    565.           2
## 10            0    605.           2
## # â€¦ with 14,520 more rows
```


---
class: bg-main1

# Interaction between quantitative and categorical variables


&lt;img src="lecture-8b-slides_files/figure-html/read-pisa-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---
class: bg-main1

.vlarge[
There are two possible models:

`\(y_i = \beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\varepsilon_i\)` (Model 1)

`\(y_i = \beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i1}*x_{i2}+\varepsilon_i\)` (Model 2)

- `\(y=\)` science score
- `\(x_1=\)` science study time
- `\(x_2=\)` science enjoyment. 

Model 2 has an interaction term. 

This means that the slope will be allowed to vary for the different levels of the categorical variables, science_fun.
]

---
class: bg-main1

# Note on modelling

.huge[
*Note:* Ordered factors are treated as "numeric" in the default model fit, so we should convert `science_fun` to be an unordered categorical variable. Also, `science_time` is heavily skewed so should be transformed.
]

---
class: bg-main1


```r
pisa_au_science_log10 &lt;- pisa_au_science %&gt;%
  mutate(log_science_time = log10(science_time)) %&gt;%
  mutate(science_fun_c = factor(science_fun, ordered = FALSE))

*mod1 &lt;- lm(science ~ log_science_time + science_fun_c,
           data = pisa_au_science_log10, 
           weights = stuweight)

*mod2 &lt;- lm(science ~ log_science_time * science_fun_c,
           data = pisa_au_science_log10, 
           weights = stuweight)
```

---
class: bg-main1


```r
tidy(mod1)
```

```
## # A tibble: 5 x 5
##   term             estimate std.error statistic   p.value
##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)         268.      14.6       18.3 1.26e- 73
## 2 log_science_time     83.8      6.18      13.6 1.31e- 41
## 3 science_fun_c2       31.8      3.15      10.1 9.28e- 24
## 4 science_fun_c3       63.1      2.80      22.5 7.17e-110
## 5 science_fun_c4      104.       3.25      32.1 3.00e-216
```

```r
tidy(mod2)
```

```
## # A tibble: 8 x 5
##   term                              estimate std.error statistic   p.value
##   &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)                      463.           43.5 10.6       2.37e-26
## 2 log_science_time                  -0.00400      18.6 -0.000215 10.00e- 1
## 3 science_fun_c2                  -152.           55.5 -2.74      6.18e- 3
## 4 science_fun_c3                  -167.           48.1 -3.46      5.33e- 4
## 5 science_fun_c4                  -122.           53.6 -2.29      2.23e- 2
## 6 log_science_time:science_fun_c2   78.6          23.8  3.31      9.33e- 4
## 7 log_science_time:science_fun_c3   98.4          20.6  4.78      1.74e- 6
## 8 log_science_time:science_fun_c4   96.9          22.8  4.25      2.11e- 5
```

---
class: bg-main1

# Five minute challenge

.huge[
- Write out the equations for both models. (Ignore the log transformation.)
- Make a **hand** sketch of both models.
]

---
class: bg-main1

# Which is the better model?


```r
glance(mod1)
```

```
## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC deviance
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
## 1     0.123         0.123  387.      385. 6.87e-311     5 -66706. 1.33e5 1.33e5   1.64e9
## # â€¦ with 1 more variable: df.residual &lt;int&gt;
```

```r
glance(mod2)
```

```
## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC deviance
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
## 1     0.125         0.125  387.      224. 2.62e-312     8 -66694. 1.33e5 1.33e5   1.64e9
## # â€¦ with 1 more variable: df.residual &lt;int&gt;
```

.vlarge[
ðŸ˜² they are both pretty bad! The interaction model (mod2) is slightly better but its really not.
]

---
class: bg-main1

# Interaction between quantitative variables

- Interactions for two quantitative variables in a model, can be thought of as allowing the paper sheet (model) to curl.

&lt;img src="lecture-8b-slides_files/figure-html/show-curves-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---
class: bg-main1

# Five minute challenge

.huge[
Using the PISA data: How does science score relate to text anxiety and gender?
- Make a plot of science by anxtest, coloured by gender. Does it look like an interaction term might be necessary?
- Fit the model with `science` score as the response and `gender` and `anxtest`. 
- Try an interaction between gender and anxtest. 
- Which is the better model?
] 

---
class: bg-main1

# Model building, Goal:

.huge[
The simplest model possible that provides similar predictive accuracy to most complex model.
]

---
class: bg-main1

# Model building, Approach:

.huge[
- Start simply, fit main effects models (single best variable, adding several more variables independently) and try to understand the effect that each has in the model. 
- Explore transformations with the aim to build a stable foundation of explanatory variables for the model.
- Check model diagnostics, residual plots.
- Explore two variable interactions, and understand effect on model.
- Explore three variable interactions.
- Use model goodness of fit to help decide on final. There may be more than one model that are almost equally as good.
]

---
class: bg-main1

# Some asides on model building

.vlarge[
- Ideally, values of explanatory variables cover all possible combinations in their domain. 
- There should *not be any association between explanatory variables*. 
- If there is, then the there is more uncertainty in the parameter estimates. 
- Its like building a table with only two legs, that table would be a bit wobbly, and unstable. 
]

---
class: bg-main1

# Some asides on model building

.vlarge[
- A work around is to first regress one explanatory variable on the other, and add the residuals from this fit to the model, instead of the original variable. 
- That is, suppose `\(X_1, X_2\)` are strongly linearly associated, then model `\(X_2\sim b_0+b_1X_1+e\)`, and use `\(e\)` (call it `\(X^*_2\)`) in the model instead of `\(X_2\)`. 
- You would then only be using the part of `\(X_2\)` that is not related to `\(X_1\)` to expand the model. 
- This approach can be used for multiple explanatory variables that are associated. 
]

---
class: bg-main1

# Your Turn:

.vlarge[
Build the best model you can for science scoreby exploring  these variables: 

- math score
- reading score
- tvs
- books
- breakfast

Feel free to choose others. (Code provided in exercise is just a sample, and needs to be modified.)
]


---
class: bg-main1

# Sentiment analysis

.huge[
Sentiment analysis tags words or phrases with an emotion, and summarises these, often as the positive or negative state, over a body of text. 
]

---
class: bg-main1

# Sentiment analysis: examples

.huge[
- Examining effect of emotional state in twitter posts
- Determining public reactions to government policy, or new product releases
- Trying to make money in the stock market by modeling social media posts on listed companies
- Evaluating product reviews on Amazon, restaurants on zomato, or travel options on TripAdvisor
]

---
class: bg-main1

# Lexicons

.huge[
The `tidytext` package has a lexicon of sentiments, based on four major sources: [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), [bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), [Loughran](https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists), [nrc](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)
]

---
class: bg-main1

# emotion

.huge[
What emotion do these words elicit in you?

- summer
- hot chips
- hug
- lose
- stolen
- smile
]

---
class: bg-main1

# Different sources of sentiment

.huge[
- The `nrc` lexicon categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 
- The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. 
- The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.
]

---
class: bg-main1

# Different sources of sentiment

```r
get_sentiments("afinn")
```

```
## # A tibble: 2,477 x 2
##    word       value
##    &lt;chr&gt;      &lt;dbl&gt;
##  1 abandon       -2
##  2 abandoned     -2
##  3 abandons      -2
##  4 abducted      -2
##  5 abduction     -2
##  6 abductions    -2
##  7 abhor         -3
##  8 abhorred      -3
##  9 abhorrent     -3
## 10 abhors        -3
## # â€¦ with 2,467 more rows
```

---
class: bg-main1

# Sentiment analysis

.huge[
- Once you have a bag of words, you need to join the sentiments dictionary to the words data. 
- Particularly the lexicon `nrc` has multiple tags per word, so you may need to use an "inner_join". 
- `inner_join()` returns all rows from x where there are matching values in y, and all columns from x and y. 
- If there are multiple matches between x and y, all combination of the matches are returned.
]

---
class: bg-main1

# Exploring sentiment in Jane Austen

.huge[
`janeaustenr` package contains the full texts, ready for analysis for for Jane Austen's 6 completed novels: 

1. "Sense and Sensibility"
2. "Pride and Prejudice"
3. "Mansfield Park"
4. "Emma"
5. "Northanger Abbey"
6. "Persuasion"
]


---
class: bg-main1

# Exploring sentiment in Jane Austen


```r
library(janeaustenr)
library(stringr)

tidy_books &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]", 
                                           ignore_case = TRUE)))) %&gt;%
  ungroup() %&gt;%
  unnest_tokens(word, text)
```

---
class: bg-main1

# Exploring sentiment in Jane Austen

```r
tidy_books
```

```
## # A tibble: 725,055 x 4
##    book                linenumber chapter word       
##    &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      
##  1 Sense &amp; Sensibility          1       0 sense      
##  2 Sense &amp; Sensibility          1       0 and        
##  3 Sense &amp; Sensibility          1       0 sensibility
##  4 Sense &amp; Sensibility          3       0 by         
##  5 Sense &amp; Sensibility          3       0 jane       
##  6 Sense &amp; Sensibility          3       0 austen     
##  7 Sense &amp; Sensibility          5       0 1811       
##  8 Sense &amp; Sensibility         10       1 chapter    
##  9 Sense &amp; Sensibility         10       1 1          
## 10 Sense &amp; Sensibility         13       1 the        
## # â€¦ with 725,045 more rows
```

---
class: bg-main1

# Count joyful words in "Emma"


```r
nrc_joy &lt;- get_sentiments("nrc") %&gt;% 
  filter(sentiment == "joy")

tidy_books %&gt;%
  filter(book == "Emma") %&gt;%
  inner_join(nrc_joy) %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 303 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 good      359
##  2 young     192
##  3 friend    166
##  4 hope      143
##  5 happy     125
##  6 love      117
##  7 deal       92
##  8 found      92
##  9 present    89
## 10 kind       82
## # â€¦ with 293 more rows
```

---
class: bg-main1

# Count joyful words in "Emma"

.huge[
"Good" is the most common joyful word, followed by "young", "friend", "hope". 

All make sense until you see "found". 

Is "found" a joyful word?
]

---
class: bg-main1

# Your turn: go to rstudio.cloud

.huge[
- What are the most common "anger" words used in Emma?
- What are the most common "surprise" words used in Emma?
]

---
class: bg-main1

# Comparing lexicons

.huge.pull-left[
- All of the lexicons have a measure of positive or negative. 
- We can tag the words in Emma by each lexicon, and see if they agree. 
]

.pull-right[

```r
nrc_pn &lt;- get_sentiments("nrc") %&gt;% 
  filter(sentiment %in% c("positive", 
                          "negative"))

emma_nrc &lt;- tidy_books %&gt;%
  filter(book == "Emma") %&gt;%
  inner_join(nrc_pn)

emma_bing &lt;- tidy_books %&gt;%
  filter(book == "Emma") %&gt;%
  inner_join(get_sentiments("bing")) 

emma_afinn &lt;- tidy_books %&gt;%
  filter(book == "Emma") %&gt;%
  inner_join(get_sentiments("afinn"))
```
]

---
class: bg-main1

# Comparing lexicons


```r
emma_nrc
```

```
## # A tibble: 13,944 x 5
##    book  linenumber chapter word       sentiment
##    &lt;fct&gt;      &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;    
##  1 Emma          15       1 clever     positive 
##  2 Emma          16       1 happy      positive 
##  3 Emma          16       1 blessings  positive 
##  4 Emma          17       1 existence  positive 
##  5 Emma          18       1 distress   negative 
##  6 Emma          21       1 marriage   positive 
##  7 Emma          22       1 mistress   negative 
##  8 Emma          22       1 mother     negative 
##  9 Emma          22       1 mother     positive 
## 10 Emma          23       1 indistinct negative 
## # â€¦ with 13,934 more rows
```

---
class: bg-main1

# Comparing lexicons


```r
emma_afinn
```

```
## # A tibble: 10,901 x 5
##    book  linenumber chapter word         value
##    &lt;fct&gt;      &lt;int&gt;   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1 Emma          15       1 clever           2
##  2 Emma          15       1 rich             2
##  3 Emma          15       1 comfortable      2
##  4 Emma          16       1 happy            3
##  5 Emma          16       1 best             3
##  6 Emma          18       1 distress        -2
##  7 Emma          20       1 affectionate     3
##  8 Emma          22       1 died            -3
##  9 Emma          24       1 excellent        3
## 10 Emma          25       1 fallen          -2
## # â€¦ with 10,891 more rows
```


---
class: bg-main1

# Comparing lexicons


```r
emma_nrc %&gt;% count(sentiment) %&gt;% mutate(n / sum(n))
```

```
## # A tibble: 2 x 3
##   sentiment     n `n/sum(n)`
##   &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;
## 1 negative   4473      0.321
## 2 positive   9471      0.679
```

```r
emma_bing %&gt;% count(sentiment) %&gt;% mutate(n / sum(n))
```

```
## # A tibble: 2 x 3
##   sentiment     n `n/sum(n)`
##   &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;
## 1 negative   4809      0.402
## 2 positive   7157      0.598
```

---
class: bg-main1

# Comparing lexicons


```r
emma_afinn %&gt;% 
  mutate(sentiment = ifelse(value &gt; 0, 
                            "positive", 
                            "negative")) %&gt;% 
  count(sentiment) %&gt;% 
  mutate(n / sum(n))
```

```
## # A tibble: 2 x 3
##   sentiment     n `n/sum(n)`
##   &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;
## 1 negative   4429      0.406
## 2 positive   6472      0.594
```

---
class: bg-main1

# Your turn: Exercise 2

.huge[
- Using your choice of lexicon (nrc, bing, or afinn) compute the proportion of positive words in each of Austen's books.
- Which book is the most positive? negative?
]


---
class: bg-main1

# Example: Simpsons

.huge[
Data from the popular animated TV series, The Simpsons, has been made available on [kaggle](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data/data). 

- `simpsons_script_lines.csv`: Contains the text spoken during each episode (including details about which character said it and where)
- `simpsons_characters.csv`: Contains character names and a character id
]

---
class: bg-main1

# The Simpsons


```r
scripts &lt;- read_csv("data/simpsons_script_lines.csv")
chs &lt;- read_csv("data/simpsons_characters.csv")
sc &lt;- left_join(scripts, chs, by = c("character_id" = "id"))

sc
```

```
## # A tibble: 157,462 x 16
##       id episode_id number raw_text timestamp_in_ms speaking_line character_id location_id
##    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;       &lt;dbl&gt;
##  1  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  2  9550         32    210 Lisa Siâ€¦          856000 TRUE                     9           3
##  3  9551         32    211 Miss Hoâ€¦          856000 TRUE                   464           3
##  4  9552         32    212 Lisa Siâ€¦          864000 TRUE                     9           3
##  5  9553         32    213 Edna Krâ€¦          864000 TRUE                    40           3
##  6  9554         32    214 Martin â€¦          877000 TRUE                    38           3
##  7  9555         32    215 Edna Krâ€¦          881000 TRUE                    40           3
##  8  9556         32    216 Bart Siâ€¦          882000 TRUE                     8           3
##  9  9557         32    217 (Apartmâ€¦          889000 FALSE                   NA         374
## 10  9558         32    218 Lisa Siâ€¦          889000 TRUE                     9         374
## # â€¦ with 157,452 more rows, and 8 more variables: raw_character_text &lt;chr&gt;,
## #   raw_location_text &lt;chr&gt;, spoken_words &lt;chr&gt;, normalized_text &lt;chr&gt;, word_count &lt;chr&gt;,
## #   name &lt;chr&gt;, normalized_name &lt;chr&gt;, gender &lt;chr&gt;
```

---
class: bg-main1

# count the number of times a character speaks


```r
sc %&gt;% count(name, sort = TRUE)
```

```
## # A tibble: 6,143 x 2
##    name                    n
##    &lt;chr&gt;               &lt;int&gt;
##  1 Homer Simpson       29945
##  2 &lt;NA&gt;                19661
##  3 Marge Simpson       14192
##  4 Bart Simpson        13894
##  5 Lisa Simpson        11573
##  6 C. Montgomery Burns  3196
##  7 Moe Szyslak          2853
##  8 Seymour Skinner      2437
##  9 Ned Flanders         2139
## 10 Grampa Simpson       1952
## # â€¦ with 6,133 more rows
```

---
class: bg-main1

# missing name?


```r
sc %&gt;% filter(is.na(name))
```

```
## # A tibble: 19,661 x 16
##       id episode_id number raw_text timestamp_in_ms speaking_line character_id location_id
##    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;       &lt;dbl&gt;
##  1  9557         32    217 (Apartmâ€¦          889000 FALSE                   NA         374
##  2  9565         32    225 (Springâ€¦          918000 FALSE                   NA           3
##  3 75766        263    106 (Moe's â€¦          497000 FALSE                   NA          15
##  4  9583         32    243 (Train â€¦          960000 FALSE                   NA         375
##  5  9604         32    264 (Simpsoâ€¦         1070000 FALSE                   NA           5
##  6  9655         33      0 (Simpsoâ€¦           84000 FALSE                   NA           5
##  7  9685         33     30 (Simpsoâ€¦          177000 FALSE                   NA           5
##  8  9686         33     31 (Simpsoâ€¦          177000 FALSE                   NA           5
##  9  9727         33     72 (Simpsoâ€¦          349000 FALSE                   NA         151
## 10  9729         33     74 (Simpsoâ€¦          355000 FALSE                   NA           5
## # â€¦ with 19,651 more rows, and 8 more variables: raw_character_text &lt;chr&gt;,
## #   raw_location_text &lt;chr&gt;, spoken_words &lt;chr&gt;, normalized_text &lt;chr&gt;, word_count &lt;chr&gt;,
## #   name &lt;chr&gt;, normalized_name &lt;chr&gt;, gender &lt;chr&gt;
```


---
class: bg-main1

# Simpsons Pre-process the text


```r
sc %&gt;%
  unnest_tokens(output = word, 
                input = spoken_words)
```

```
## # A tibble: 1,355,370 x 16
##       id episode_id number raw_text timestamp_in_ms speaking_line character_id location_id
##    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;       &lt;dbl&gt;
##  1  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  2  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  3  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  4  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  5  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  6  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  7  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  8  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  9  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
## 10  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
## # â€¦ with 1,355,360 more rows, and 8 more variables: raw_character_text &lt;chr&gt;,
## #   raw_location_text &lt;chr&gt;, normalized_text &lt;chr&gt;, word_count &lt;chr&gt;, name &lt;chr&gt;,
## #   normalized_name &lt;chr&gt;, gender &lt;chr&gt;, word &lt;chr&gt;
```

---
class: bg-main1

# Simpsons Pre-process the text



```r
sc %&gt;%
  unnest_tokens(output = word, 
                input = spoken_words) %&gt;%
  anti_join(stop_words)
```

```
## # A tibble: 511,869 x 16
##       id episode_id number raw_text timestamp_in_ms speaking_line character_id location_id
##    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;       &lt;dbl&gt;
##  1  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  2  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  3  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  4  9549         32    209 Miss Hoâ€¦          848000 TRUE                   464           3
##  5  9550         32    210 Lisa Siâ€¦          856000 TRUE                     9           3
##  6  9551         32    211 Miss Hoâ€¦          856000 TRUE                   464           3
##  7  9551         32    211 Miss Hoâ€¦          856000 TRUE                   464           3
##  8  9551         32    211 Miss Hoâ€¦          856000 TRUE                   464           3
##  9  9551         32    211 Miss Hoâ€¦          856000 TRUE                   464           3
## 10  9551         32    211 Miss Hoâ€¦          856000 TRUE                   464           3
## # â€¦ with 511,859 more rows, and 8 more variables: raw_character_text &lt;chr&gt;,
## #   raw_location_text &lt;chr&gt;, normalized_text &lt;chr&gt;, word_count &lt;chr&gt;, name &lt;chr&gt;,
## #   normalized_name &lt;chr&gt;, gender &lt;chr&gt;, word &lt;chr&gt;
```

---
class: bg-main1

# Simpsons Pre-process the text


```r
sc %&gt;%
  unnest_tokens(output = word, 
                input = spoken_words) %&gt;%
  anti_join(stop_words) %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(!is.na(word))
```

```
## # A tibble: 41,891 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 hey      4366
##  2 homer    4328
##  3 bart     3434
##  4 uh       3090
##  5 yeah     2997
##  6 simpson  2846
##  7 marge    2786
##  8 gonna    2639
##  9 dad      2521
## 10 time     2508
## # â€¦ with 41,881 more rows
```

---
class: bg-main1

# Simpsons Pre-process the text


```r
sc_top_20 &lt;- sc %&gt;%
  unnest_tokens(output = word, 
                input = spoken_words) %&gt;%
  anti_join(stop_words) %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(!is.na(word)) %&gt;%
  mutate(word = factor(word, 
                       levels = rev(unique(word)))) %&gt;%
  top_n(20)
```

---
class: bg-main1

# Simpsons plot most common words

.left-code[

```r
ggplot(sc_top_20,
       aes(x = word, 
           y = n)) +
  geom_col() +
  labs(x = '', 
       y = 'count', 
       title = 'Top 20 words') +
  coord_flip() + 
  theme_bw()
```
]

.right-plot[
&lt;img src="lecture-8b-slides_files/figure-html/process-simpsons-s5-out-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
class: bg-main1

# Tag the words with sentiments

.huge[
Using AFINN words will be tagged on a negative to positive scale of -1 to 5.
]

.pull-left[

```r
sc_word &lt;- sc %&gt;%
  unnest_tokens(output = word, 
                input = spoken_words) %&gt;%
  anti_join(stop_words) %&gt;%
  count(name, word) %&gt;%
  filter(!is.na(word))
```
]

.pull-right[

```r
sc_word
```

```
## # A tibble: 220,838 x 3
##    name            word            n
##    &lt;chr&gt;           &lt;chr&gt;       &lt;int&gt;
##  1 '30s Reporter   burns           1
##  2 '30s Reporter   kinda           1
##  3 '30s Reporter   sensational     1
##  4 1-Year-Old Bart beer            1
##  5 1-Year-Old Bart daddy           5
##  6 1-Year-Old Bart fat             1
##  7 1-Year-Old Bart moustache       1
##  8 1-Year-Old Bart nice            1
##  9 1-Year-Old Bart smell           1
## 10 1-Year-Old Bart yell            1
## # â€¦ with 220,828 more rows
```
]

---
class: bg-main1


```r
sc_s &lt;- sc_word %&gt;% 
  inner_join(get_sentiments("afinn"), by = "word")

sc_s
```

```
## # A tibble: 26,688 x 4
##    name              word       n value
##    &lt;chr&gt;             &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;
##  1 1-Year-Old Bart   nice       1     3
##  2 10-Year-Old Homer chance     1     2
##  3 10-Year-Old Homer cool       1     1
##  4 10-Year-Old Homer die        1    -3
##  5 10-Year-Old Homer died       1    -3
##  6 10-Year-Old Homer dreams     1     1
##  7 10-Year-Old Homer happy      1     3
##  8 10-Year-Old Homer heaven     1     2
##  9 10-Year-Old Homer hell       1    -4
## 10 10-Year-Old Homer kiss       1     2
## # â€¦ with 26,678 more rows
```

---
class: bg-main1

# Examine Simpsons characters


```r
sc_s %&gt;% 
  group_by(name) %&gt;% 
  summarise(m = mean(value)) %&gt;% 
  arrange(desc(m))
```

```
## # A tibble: 3,409 x 2
##    name                    m
##    &lt;chr&gt;               &lt;dbl&gt;
##  1 2nd Sportscaster        4
##  2 4-h Judge               4
##  3 7-Year-Old Brockman     4
##  4 ALEPPO                  4
##  5 All Kids                4
##  6 Applicants              4
##  7 Australian              4
##  8 Bill James              4
##  9 Canadian Player         4
## 10 Carl Kasell             4
## # â€¦ with 3,399 more rows
```

---
class: bg-main1

# Examine Simpsons characters: Focus on the main characters.



```r
keep &lt;- sc %&gt;% count(name, 
                     sort=TRUE) %&gt;%
  filter(!is.na(name)) %&gt;%
  filter(n &gt; 999)

sc_s %&gt;% 
  filter(name %in% keep$name) %&gt;% 
  group_by(name) %&gt;% 
  summarise(m = mean(value)) %&gt;% 
  arrange(m)
```

```
## # A tibble: 16 x 2
##    name                        m
##    &lt;chr&gt;                   &lt;dbl&gt;
##  1 Nelson Muntz           -0.519
##  2 Grampa Simpson         -0.429
##  3 Homer Simpson          -0.428
##  4 Bart Simpson           -0.391
##  5 Chief Wiggum           -0.388
##  6 Lisa Simpson           -0.388
##  7 Marge Simpson          -0.344
##  8 Apu Nahasapeemapetilon -0.339
##  9 Moe Szyslak            -0.313
## 10 C. Montgomery Burns    -0.310
## 11 Ned Flanders           -0.265
## 12 Milhouse Van Houten    -0.244
## 13 Krusty the Clown       -0.218
## 14 Seymour Skinner        -0.194
## 15 Waylon Smithers        -0.167
## 16 Lenny Leonard          -0.154
```

---
class: bg-main1

# Your turn: Exercise 3

.huge[
1. Bart Simpson is featured at various ages. How has the sentiment of his words changed over his life?

2. Repeat the sentiment analysis with the NRC lexicon. What character is the most "angry"? "joyful"?
]

---
class: bg-main1

# (if time) Example: AFL Finals tweets

.huge[
The `rtweet` package allows you to pull tweets from the archive. It gives only the last 6-9 days worth of data. You need to have a twitter account, and you need to create an app (its really basic) in order to pull twitter data. The instructions that come from this package (https://rtweet.info) are pretty simple to follow.
]

---
class: bg-main1

# (if time) Example: AFL Finals tweets

.huge[
Given that it is AFL final week, I thought it might be interesting to look at tweets that use the hashtag "#AFLFinals". Once you have a developer account, this is as simple as 
]
```
afl &lt;- search_tweets(
  "#AFLFinals", n = 20000, include_rts = FALSE
)
```

---
class: bg-main1

.huge[
Here is the data collected in the previous year's AFL finals.
]


```r
afl &lt;- read_rds("data/afl_twitter_past.rds")
afl
```

```
## # A tibble: 9,900 x 88
##    user_id status_id created_at          screen_name text  source display_text_wiâ€¦
##  * &lt;chr&gt;   &lt;chr&gt;     &lt;dttm&gt;              &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;
##  1 124305â€¦ 10402046â€¦ 2018-09-13 11:45:00 JimWilsonTV @GWSâ€¦ Twittâ€¦              129
##  2 124305â€¦ 10385921â€¦ 2018-09-09 00:57:25 JimWilsonTV No tâ€¦ Twittâ€¦              113
##  3 124305â€¦ 10380010â€¦ 2018-09-07 09:48:37 JimWilsonTV Thanâ€¦ Twittâ€¦              162
##  4 124305â€¦ 10382372â€¦ 2018-09-08 01:27:11 JimWilsonTV Canâ€™â€¦ Twittâ€¦              139
##  5 124305â€¦ 10391266â€¦ 2018-09-10 12:21:15 JimWilsonTV On yâ€¦ Twittâ€¦              163
##  6 124305â€¦ 10383958â€¦ 2018-09-08 11:57:28 JimWilsonTV Greaâ€¦ Twittâ€¦              101
##  7 124305â€¦ 10374607â€¦ 2018-09-05 22:01:31 JimWilsonTV @NThâ€¦ Twittâ€¦               69
##  8 124305â€¦ 10371973â€¦ 2018-09-05 04:34:47 JimWilsonTV 16 yâ€¦ Twittâ€¦              176
##  9 330082â€¦ 10402039â€¦ 2018-09-13 11:41:57 AlexWard64  I'veâ€¦ Twittâ€¦              137
## 10 303429â€¦ 10402010â€¦ 2018-09-13 11:30:30 HAWKSHEROES I'm â€¦ Twittâ€¦              119
## # â€¦ with 9,890 more rows, and 81 more variables: reply_to_status_id &lt;chr&gt;,
## #   reply_to_user_id &lt;chr&gt;, reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;,
## #   favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, hashtags &lt;list&gt;, symbols &lt;list&gt;,
## #   urls_url &lt;list&gt;, urls_t.co &lt;list&gt;, urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;,
## #   media_t.co &lt;list&gt;, media_expanded_url &lt;list&gt;, media_type &lt;list&gt;,
## #   ext_media_url &lt;list&gt;, ext_media_t.co &lt;list&gt;, ext_media_expanded_url &lt;list&gt;,
## #   ext_media_type &lt;chr&gt;, mentions_user_id &lt;list&gt;, mentions_screen_name &lt;list&gt;,
## #   lang &lt;chr&gt;, quoted_status_id &lt;chr&gt;, quoted_text &lt;chr&gt;, quoted_created_at &lt;dttm&gt;,
## #   quoted_source &lt;chr&gt;, quoted_favorite_count &lt;int&gt;, quoted_retweet_count &lt;int&gt;,
## #   quoted_user_id &lt;chr&gt;, quoted_screen_name &lt;chr&gt;, quoted_name &lt;chr&gt;,
## #   quoted_followers_count &lt;int&gt;, quoted_friends_count &lt;int&gt;,
## #   quoted_statuses_count &lt;int&gt;, quoted_location &lt;chr&gt;, quoted_description &lt;chr&gt;,
## #   quoted_verified &lt;lgl&gt;, retweet_status_id &lt;chr&gt;, retweet_text &lt;chr&gt;,
## #   retweet_created_at &lt;dttm&gt;, retweet_source &lt;chr&gt;, retweet_favorite_count &lt;int&gt;,
## #   retweet_retweet_count &lt;int&gt;, retweet_user_id &lt;chr&gt;, retweet_screen_name &lt;chr&gt;,
## #   retweet_name &lt;chr&gt;, retweet_followers_count &lt;int&gt;, retweet_friends_count &lt;int&gt;,
## #   retweet_statuses_count &lt;int&gt;, retweet_location &lt;chr&gt;, retweet_description &lt;chr&gt;,
## #   retweet_verified &lt;lgl&gt;, place_url &lt;chr&gt;, place_name &lt;chr&gt;, place_full_name &lt;chr&gt;,
## #   place_type &lt;chr&gt;, country &lt;chr&gt;, country_code &lt;chr&gt;, geo_coords &lt;list&gt;,
## #   coords_coords &lt;list&gt;, bbox_coords &lt;list&gt;, status_url &lt;chr&gt;, name &lt;chr&gt;,
## #   location &lt;chr&gt;, description &lt;chr&gt;, url &lt;chr&gt;, protected &lt;lgl&gt;, followers_count &lt;int&gt;,
## #   friends_count &lt;int&gt;, listed_count &lt;int&gt;, statuses_count &lt;int&gt;,
## #   favourites_count &lt;int&gt;, account_created_at &lt;dttm&gt;, verified &lt;lgl&gt;, profile_url &lt;chr&gt;,
## #   profile_expanded_url &lt;chr&gt;, account_lang &lt;chr&gt;, profile_banner_url &lt;chr&gt;,
## #   profile_background_url &lt;chr&gt;, profile_image_url &lt;chr&gt;
```

---
class: bg-main1

# Your turn

.huge[
- When was the final played last year?
- What is the range of dates of this data?
- Who is the most frequent tweeter using this hashtag?
- Are there some days that have more tweets than others?
- Are there some hours of the day that are more common tweet times?
]

---
class: bg-main1

# Your Turn: Sentiment analysis

.huge[
We need to break text of each tweet into words, tag words with sentiments, and make a cumulative score for each tweet.

- Which tweeter is the most positive? negative?
- Is there a day that spirits were higher in the tweets? Or when tweets were more negative?
- Does the tweeter `aflratings` have a trend in positivity or negativity?
]


---
class: bg-main1

## Share and share alike

&lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
